{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ffaa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 118, 157, 128)     4736      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 59, 78, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 59, 78, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 59, 78, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 29, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 29, 39, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 29, 39, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 19, 32)        128       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8512)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               1702600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 1,830,206\n",
      "Trainable params: 1,830,078\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "model_rgb = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(128, kernel_size=(3, 4), input_shape=(120, 160, 3), strides=(1, 1), padding='valid',\n",
    "                      activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(10, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_rgb.summary()\n",
    "model_rgb.load_weights('rgb_only_new_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23098f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4]\n",
      "[4, 4, 4, 5, 5, 5, 0, 6, 5, 5]\n",
      "[4]\n",
      "[5, 5, 5, 5, 0, 5, 6, 6, 6, 5]\n",
      "[5]\n",
      "[5, 4, 4, 4, 0, 6, 8, 5, 0, 5]\n",
      "[5]\n",
      "[5, 5, 5, 1, 0, 0, 5, 0, 0, 0]\n",
      "[5]\n",
      "[0, 5, 1, 4, 5, 4, 2, 0, 5, 0]\n",
      "[0]\n",
      "[0, 5, 0, 0, 0, 7, 0, 0, 0, 5]\n",
      "[0]\n",
      "[2, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[2]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "[4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_18452/3640629580.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m30\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m             \u001B[0mpipeline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait_for_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m         \u001B[0mframes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait_for_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mdepth_frame\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mframes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_depth_frame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    results = []\n",
    "    pipeline.start(config)\n",
    "    while True:\n",
    "        for i in range(30):\n",
    "            pipeline.wait_for_frames()\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        tmp = cv2.resize(color_image, (160, 120), interpolation=cv2.INTER_AREA)\n",
    "        res = model_rgb.predict(tmp.reshape((1, tmp.shape[0], tmp.shape[1], tmp.shape[2])))\n",
    "\n",
    "        results.append(np.argmax(res[0]))\n",
    "\n",
    "        if len(results) == 10:\n",
    "            print(results)\n",
    "            print(np.unique(results[0]))\n",
    "            results = []\n",
    "\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "        color_colormap_dim = color_image.shape\n",
    "\n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]),\n",
    "                                             interpolation=cv2.INTER_AREA)\n",
    "            images = np.hstack((resized_color_image, depth_colormap))\n",
    "        else:\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        cv2.waitKey(1)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d872e18f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stop() cannot be called before start()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_18452/4240322211.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpipeline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m: stop() cannot be called before start()"
     ]
    }
   ],
   "source": [
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57488c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_depth = keras.models.load_model('video_depth_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f22532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 38, 118, 157, 8)   872       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 19, 59, 78, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 19, 59, 78, 16)    3472      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 9, 29, 39, 16)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 29, 39, 16)     64        \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 9, 29, 39, 8)      3464      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 14, 19, 8)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 14, 19, 8)      32        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8512)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8512)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                425650    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 434,700\n",
      "Trainable params: 434,652\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_depth.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bdb57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "PERCENT = 25\n",
    "sequence_length = 40\n",
    "\n",
    "colorizer = rs.colorizer()\n",
    "colorizer.set_option(rs.option.color_scheme, 0)\n",
    "\n",
    "\n",
    "def resize_image(img):\n",
    "    width = int(img.shape[1] * PERCENT / 100)\n",
    "    height = int(img.shape[0] * PERCENT / 100)\n",
    "    dim = (width, height)\n",
    "    resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8d267e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MFCreateDeviceSource(_device_attrs, &_source) returned: HResult 0x80070003: \"Le chemin d’accès spécifié est introuvable.\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22112/3980679273.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Start streaming\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mpipeline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0msequence_rgb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mpredictions_rgb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: MFCreateDeviceSource(_device_attrs, &_source) returned: HResult 0x80070003: \"Le chemin d’accès spécifié est introuvable.\""
     ]
    }
   ],
   "source": [
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "sequence_rgb = []\n",
    "predictions_rgb = []\n",
    "sequence_depth = []\n",
    "predictions_depth = []\n",
    "threshold = 0.7\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        color_image = resize_image(color_image)\n",
    "        depth_image = resize_image(cv2.resize(np.asanyarray(colorizer.colorize(depth_frame).get_data()), (640, 480),\n",
    "                                              interpolation=cv2.INTER_AREA))\n",
    "\n",
    "        sequence_rgb.append(color_image)\n",
    "        sequence_rgb = sequence_rgb[-sequence_length:]\n",
    "\n",
    "        sequence_depth.append(depth_image)\n",
    "        sequence_depth = sequence_depth[-sequence_length:]\n",
    "        if len(sequence_depth) == 40:\n",
    "            break\n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e14bfeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array(sequence_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a807bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 120, 160, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb924da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "movements = np.array(['scroll_right', 'scroll_left', 'scroll_up', 'scroll_down', 'zoom_in', 'zoom_out'])\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cde8c87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loading done! Starting train set creation...\n"
     ]
    }
   ],
   "source": [
    "root = 'video_dataset/depth'\n",
    "label_map = {label: num for num, label in enumerate(movements)}\n",
    "sequences, labels = [], []\n",
    "for movement in movements:\n",
    "    for dirpath, dirnames, files in os.walk(os.path.join(root, movement)):\n",
    "        sequence = []\n",
    "        for file_name in files:\n",
    "            img = cv2.imread(os.path.join(dirpath, file_name))\n",
    "            sequence.append(img)\n",
    "        if len(sequence) > 0:\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label_map[movement])\n",
    "print('Image loading done! Starting train set creation...')\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train_depth, X_val_depth, y_train_depth, y_val_depth = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05952854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loading done! Starting train set creation...\n"
     ]
    }
   ],
   "source": [
    "root = 'video_dataset/rgb'\n",
    "label_map = {label: num for num, label in enumerate(movements)}\n",
    "sequences, labels = [], []\n",
    "for movement in movements:\n",
    "    for dirpath, dirnames, files in os.walk(os.path.join(root, movement)):\n",
    "        sequence = []\n",
    "        for file_name in files:\n",
    "            img = cv2.imread(os.path.join(dirpath, file_name))\n",
    "            sequence.append(img)\n",
    "        if len(sequence) > 0:\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label_map[movement])\n",
    "print('Image loading done! Starting train set creation...')\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train_rgb, X_val_rgb, y_train_rgb, y_val_rgb = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c2d5169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[386,   0],\n",
       "        [  0,  83]],\n",
       "\n",
       "       [[377,   1],\n",
       "        [  0,  91]],\n",
       "\n",
       "       [[404,   0],\n",
       "        [  0,  65]],\n",
       "\n",
       "       [[389,   1],\n",
       "        [  0,  79]],\n",
       "\n",
       "       [[390,   0],\n",
       "        [  1,  78]],\n",
       "\n",
       "       [[397,   0],\n",
       "        [  1,  71]]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rgb = keras.models.load_model('video_rgb_weights.h5')\n",
    "model_depth = keras.models.load_model('video_depth_weights.h5')\n",
    "\n",
    "yhat_rgb = model_rgb.predict(X_val_rgb)\n",
    "ytrue_rgb = np.argmax(y_val_rgb, axis=1).tolist()\n",
    "yhat_rgb = np.argmax(yhat_rgb, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue_rgb, yhat_rgb, labels=movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8690f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        83\n",
      "           1       0.99      1.00      0.99        91\n",
      "           2       1.00      1.00      1.00        65\n",
      "           3       0.99      1.00      0.99        79\n",
      "           4       1.00      0.99      0.99        79\n",
      "           5       1.00      0.99      0.99        72\n",
      "\n",
      "    accuracy                           1.00       469\n",
      "   macro avg       1.00      1.00      1.00       469\n",
      "weighted avg       1.00      1.00      1.00       469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytrue_rgb, yhat_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c47e00c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python39\\lib\\site-packages\\numpy\\lib\\arraysetops.py:576: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22112/846637409.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mytrue_depth\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_train_depth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0myhat_depth\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0myhat_depth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mmultilabel_confusion_matrix\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mytrue_depth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0myhat_depth\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmovement\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001B[0m in \u001B[0;36mmultilabel_confusion_matrix\u001B[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001B[0m\n\u001B[0;32m    499\u001B[0m         \u001B[0mle\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mLabelEncoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    500\u001B[0m         \u001B[0mle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 501\u001B[1;33m         \u001B[0my_true\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    502\u001B[0m         \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    503\u001B[0m         \u001B[0msorted_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclasses_\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, y)\u001B[0m\n\u001B[0;32m    136\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 138\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_encode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muniques\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclasses_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0minverse_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\_encode.py\u001B[0m in \u001B[0;36m_encode\u001B[1;34m(values, uniques, check_unknown)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    186\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcheck_unknown\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 187\u001B[1;33m             \u001B[0mdiff\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_check_unknown\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muniques\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    188\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mdiff\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"y contains previously unseen labels: {str(diff)}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\python39\\lib\\site-packages\\sklearn\\utils\\_encode.py\u001B[0m in \u001B[0;36m_check_unknown\u001B[1;34m(values, known_values, return_mask)\u001B[0m\n\u001B[0;32m    259\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    260\u001B[0m         \u001B[1;31m# check for nans in the known_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 261\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mknown_values\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    262\u001B[0m             \u001B[0mdiff_is_nan\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdiff\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    263\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mdiff_is_nan\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "yhat_depth = model_depth.predict(X_train_depth)\n",
    "ytrue_depth = np.argmax(y_train_depth, axis=1).tolist()\n",
    "yhat_depth = np.argmax(yhat_depth, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue_depth, yhat_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "340212cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       176\n",
      "           1       1.00      1.00      1.00       186\n",
      "           2       0.99      1.00      1.00       177\n",
      "           3       1.00      0.98      0.99       188\n",
      "           4       1.00      1.00      1.00       177\n",
      "           5       0.99      1.00      0.99       188\n",
      "\n",
      "    accuracy                           1.00      1092\n",
      "   macro avg       1.00      1.00      1.00      1092\n",
      "weighted avg       1.00      1.00      1.00      1092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytrue_depth, yhat_depth))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}